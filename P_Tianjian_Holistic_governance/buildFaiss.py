from langchain_community.vectorstores import FAISS
import torch
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import (
    PyPDFLoader,
    UnstructuredWordDocumentLoader,
    UnstructuredExcelLoader,
)
import os

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
embedding_model = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-zh-v1.5",
    model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"}
)

# é€’å½’åŠ è½½æ–‡æ¡£ï¼Œå¹¶å°†ç›¸å¯¹è·¯å¾„å†™å…¥metadata
def load_documents_from_folder(folder_path: str):
    docs = []
    for root, _, files in os.walk(folder_path):  # é€’å½’æ‰«æ
        for filename in files:
            file_path = os.path.join(root, filename)

            # æŒ‰æ‰©å±•åé€‰æ‹©loader
            if filename.endswith(".pdf"):
                loader = PyPDFLoader(file_path)
            elif filename.endswith(".docx"):
                loader = UnstructuredWordDocumentLoader(file_path)
            elif filename.endswith(".xlsx") or filename.endswith(".xls"):
                loader = UnstructuredExcelLoader(file_path)
            else:
                continue

            # åŠ è½½æ–‡ä»¶
            loaded_docs = loader.load()

            # åœ¨ metadata ä¸­è®°å½•ç›¸å¯¹è·¯å¾„ï¼ˆä»æ ¹ç›®å½•å¼€å§‹ï¼‰
            rel_path = os.path.relpath(file_path, folder_path)
            for d in loaded_docs:
                d.metadata["source"] = rel_path

            docs.extend(loaded_docs)
    return docs

# æ„å»ºFAISSå‘é‡åº“
def build_faiss_index_from_folder(folder_path: str, index_save_path: str):
    print("ğŸ“‚ åŠ è½½æ³•è§„æ–‡æ¡£ä¸­...")
    raw_docs = load_documents_from_folder(folder_path)

    print(f"ğŸ“„ å…±åŠ è½½ {len(raw_docs)} ä¸ªåŸå§‹æ–‡æ¡£ç‰‡æ®µ")

    print("âœ‚ï¸ åˆ‡åˆ†æ–‡æ¡£ä¸ºç‰‡æ®µ...")
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    split_docs = splitter.split_documents(raw_docs)

    print(f"ğŸ“„ åˆ‡åˆ†åå¾—åˆ° {len(split_docs)} ä¸ªç‰‡æ®µ")

    print("ğŸ” æ„å»ºåµŒå…¥å‘é‡...")
    embeddings = embedding_model

    print("ğŸ’¾ æ„å»º FAISS å‘é‡æ•°æ®åº“...")
    vectordb = FAISS.from_documents(split_docs, embeddings)

    print(f"âœ… ä¿å­˜å‘é‡æ•°æ®åº“è‡³ï¼š{index_save_path}")
    vectordb.save_local(index_save_path)
    return vectordb


if __name__ == "__main__":
    folder = "./test/rule_files"
    save_path = "./test/faiss_law_index"
    build_faiss_index_from_folder(folder, save_path)
