import os
import torch
import pandas as pd
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, UnstructuredWordDocumentLoader
from langchain.docstore.document import Document

# ===== 1. é…ç½®åµŒå…¥æ¨¡å‹ =====
embedding_model = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-zh-v1.5",
    model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"},
    encode_kwargs={"normalize_embeddings": True}  # å®˜æ–¹å»ºè®®å½’ä¸€åŒ–
)

# ===== 2. åŠ è½½æ–‡æ¡£ =====
def load_documents_from_folder(folder_path: str):
    docs = []
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)

        if filename.lower().endswith(".pdf"):
            try:
                loader = PyPDFLoader(file_path)
                docs.extend(loader.load())
            except Exception as e:
                print(f"è¯»å– PDF å¤±è´¥: {filename}, é”™è¯¯: {e}")

        elif filename.lower().endswith(".docx"):
            try:
                loader = UnstructuredWordDocumentLoader(file_path)
                docs.extend(loader.load())
            except Exception as e:
                print(f"è¯»å– Word å¤±è´¥: {filename}, é”™è¯¯: {e}")

        elif filename.lower().endswith((".xlsx", ".xls")):
            try:
                # è¯»å–æ‰€æœ‰ sheetï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                df_sheets = pd.read_excel(file_path, sheet_name=None, engine="openpyxl", dtype=str)
                for sheet_name, sheet_df in df_sheets.items(): 
                    try:
                        sheet_df = sheet_df.fillna("")
                        text = sheet_df.to_string(index=False)
                        docs.append(Document(
                            page_content=text,
                            metadata={"source": f"{filename} - {sheet_name}"}
                        ))
                    except Exception as e:
                        print(f"âš ï¸ Sheet è¯»å–å¤±è´¥: {filename} - {sheet_name}, é”™è¯¯: {e}")
            except Exception as e:
                print(f"è¯»å– Excel å¤±è´¥: {filename}, é”™è¯¯: {e}")

        else:
            print(f"è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {filename}")

    return docs

# ===== 3. æ„å»º FAISS ç´¢å¼• =====
def build_faiss_index_from_folder(folder_path: str, index_save_path: str):
    print("ğŸ“‚ æ­£åœ¨åŠ è½½æ³•è§„æ–‡æ¡£...")
    raw_docs = load_documents_from_folder(folder_path)
    print(f"ğŸ“„ å…±åŠ è½½ {len(raw_docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")

    if not raw_docs:
        print("âŒ æœªåŠ è½½åˆ°ä»»ä½•æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å¤¹è·¯å¾„æˆ–æ–‡ä»¶æ ¼å¼")
        return None

    print("âœ‚ æ­£åœ¨åˆ‡åˆ†æ–‡æ¡£ä¸ºç‰‡æ®µ...")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=512,
        chunk_overlap=64,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", " ", ""]
    )
    split_docs = splitter.split_documents(raw_docs)
    print(f"ğŸ“„ åˆ‡åˆ†åå¾—åˆ° {len(split_docs)} ä¸ªæ–‡æ¡£å—")

    if not split_docs:
        print("âŒ æ²¡æœ‰æ–‡æ¡£å—å¯ç”¨äºæ„å»ºç´¢å¼•ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºç©º")
        return None

    print("ğŸ” æ­£åœ¨æ„å»ºåµŒå…¥å‘é‡...")
    embeddings = embedding_model

    print("ğŸ“¦ æ­£åœ¨æ„å»º FAISS å‘é‡æ•°æ®åº“...")
    vectordb = FAISS.from_documents(split_docs, embeddings)

    print(f"ğŸ’¾ ä¿å­˜å‘é‡æ•°æ®åº“è‡³ï¼š{index_save_path}")
    os.makedirs(index_save_path, exist_ok=True)
    vectordb.save_local(index_save_path)

    print("âœ… å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼")
    return vectordb


if __name__ == "__main__":
    # ======== é…ç½®è·¯å¾„ ========
    docs_folder = "./test/rule"       # ä½ çš„æ³•è§„æ–‡æ¡£æ‰€åœ¨æ–‡ä»¶å¤¹
    index_folder = "./faiss_index"    # ä¿å­˜ FAISS ç´¢å¼•çš„æ–‡ä»¶å¤¹

    # æ„å»ºç´¢å¼•
    build_faiss_index_from_folder(docs_folder, index_folder)
